{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# BAZI-GPT Fine-tuning Notebook for GPT-3.5-turbo-0125\n",
    "\n",
    "This notebook provides a complete workflow for fine-tuning GPT-3.5-turbo-0125 to create a specialized Four Pillars of Destiny (ÂÖ´Â≠ó) consultant using the translated_csv.csv dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Setup and Dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install required packages\n",
    "!pip install openai>=1.0.0 tqdm pandas jsonlines python-dotenv\n",
    "\n",
    "import os\n",
    "import json\n",
    "import time\n",
    "import pandas as pd\n",
    "import jsonlines\n",
    "from datetime import datetime\n",
    "from openai import OpenAI\n",
    "from tqdm import tqdm\n",
    "import random"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Configuration"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.1 Model Selection and Troubleshooting\n",
    "\n",
    "**Your previous fine-tuning job failed. Here are the recommended fixes:**\n",
    "\n",
    "### üîß **Common Issues and Solutions:**\n",
    "\n",
    "1. **Model Compatibility**: Changed from `gpt-4o-mini-2024-07-18` to `gpt-3.5-turbo-0125` (more stable)\n",
    "2. **Batch Size**: Reduced from 4 to 1 (prevents memory issues)\n",
    "3. **Learning Rate**: Lowered from 0.5 to 0.3 (better convergence)\n",
    "\n",
    "### üìã **Available Fine-tuning Models:**\n",
    "- ‚úÖ **`gpt-3.5-turbo-0125`** (Recommended - most stable)\n",
    "- ‚úÖ **`gpt-3.5-turbo-1106`** (Alternative option)\n",
    "- ‚úÖ **`gpt-4o-mini-2024-07-18`** (Your original choice - can be unstable)\n",
    "\n",
    "### üîç **Data Validation Steps:**\n",
    "The next cells will validate your data format to ensure compatibility."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data validation functions loaded. These will be used after data conversion.\n"
     ]
    }
   ],
   "source": [
    "# Data Validation and Fix Common Issues\n",
    "def validate_training_data(conversations):\n",
    "    \"\"\"Validate training data and fix common issues\"\"\"\n",
    "    print(\"üîç Validating training data...\")\n",
    "    \n",
    "    issues_found = []\n",
    "    fixed_conversations = []\n",
    "    \n",
    "    for i, conv in enumerate(conversations):\n",
    "        try:\n",
    "            # Check basic structure\n",
    "            if not isinstance(conv, dict) or \"messages\" not in conv:\n",
    "                issues_found.append(f\"Row {i}: Missing 'messages' key\")\n",
    "                continue\n",
    "                \n",
    "            messages = conv[\"messages\"]\n",
    "            if len(messages) < 3:\n",
    "                issues_found.append(f\"Row {i}: Need at least 3 messages (system, user, assistant)\")\n",
    "                continue\n",
    "            \n",
    "            # Check message roles\n",
    "            expected_roles = [\"system\", \"user\", \"assistant\"]\n",
    "            for j, msg in enumerate(messages[:3]):\n",
    "                if msg.get(\"role\") != expected_roles[j]:\n",
    "                    issues_found.append(f\"Row {i}: Message {j} should have role '{expected_roles[j]}'\")\n",
    "                    continue\n",
    "            \n",
    "            # Check content length\n",
    "            for j, msg in enumerate(messages):\n",
    "                content = msg.get(\"content\", \"\")\n",
    "                if not content or len(content.strip()) == 0:\n",
    "                    issues_found.append(f\"Row {i}, Message {j}: Empty content\")\n",
    "                    continue\n",
    "                \n",
    "                # Check for extremely long content that might cause issues\n",
    "                if len(content) > 8192:  # Token limit consideration\n",
    "                    issues_found.append(f\"Row {i}, Message {j}: Content too long ({len(content)} chars)\")\n",
    "                    # Truncate content\n",
    "                    msg[\"content\"] = content[:8000] + \"...\"\n",
    "                    \n",
    "            # If we get here, the conversation is valid or fixed\n",
    "            fixed_conversations.append(conv)\n",
    "            \n",
    "        except Exception as e:\n",
    "            issues_found.append(f\"Row {i}: Error - {str(e)}\")\n",
    "    \n",
    "    print(f\"‚úÖ Validation complete:\")\n",
    "    print(f\"   Original conversations: {len(conversations)}\")\n",
    "    print(f\"   Valid conversations: {len(fixed_conversations)}\")\n",
    "    print(f\"   Issues found: {len(issues_found)}\")\n",
    "    \n",
    "    if issues_found:\n",
    "        print(\"\\n‚ö†Ô∏è  Issues found:\")\n",
    "        for issue in issues_found[:10]:  # Show first 10 issues\n",
    "            print(f\"   - {issue}\")\n",
    "        if len(issues_found) > 10:\n",
    "            print(f\"   ... and {len(issues_found) - 10} more issues\")\n",
    "    \n",
    "    return fixed_conversations, issues_found\n",
    "\n",
    "# Function to check file format compatibility\n",
    "def check_jsonl_format(filename):\n",
    "    \"\"\"Check if JSONL file is properly formatted\"\"\"\n",
    "    print(f\"\\nüîç Checking {filename} format...\")\n",
    "    \n",
    "    try:\n",
    "        with open(filename, 'r', encoding='utf-8') as f:\n",
    "            line_count = 0\n",
    "            error_lines = []\n",
    "            \n",
    "            for i, line in enumerate(f):\n",
    "                line_count += 1\n",
    "                try:\n",
    "                    data = json.loads(line.strip())\n",
    "                    # Check if it has the expected structure\n",
    "                    if \"messages\" not in data:\n",
    "                        error_lines.append(f\"Line {i+1}: Missing 'messages' key\")\n",
    "                except json.JSONDecodeError as e:\n",
    "                    error_lines.append(f\"Line {i+1}: JSON decode error - {str(e)}\")\n",
    "                \n",
    "                if len(error_lines) > 5:  # Stop after 5 errors\n",
    "                    break\n",
    "            \n",
    "        print(f\"   Lines checked: {line_count}\")\n",
    "        print(f\"   Errors found: {len(error_lines)}\")\n",
    "        \n",
    "        if error_lines:\n",
    "            print(\"   First few errors:\")\n",
    "            for error in error_lines:\n",
    "                print(f\"     - {error}\")\n",
    "        else:\n",
    "            print(\"   ‚úÖ File format looks good!\")\n",
    "            \n",
    "        return len(error_lines) == 0\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"   ‚ùå Error reading file: {e}\")\n",
    "        return False\n",
    "\n",
    "print(\"Data validation functions loaded. These will be used after data conversion.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set your OpenAI API key\n",
    "import os\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "# Load environment variables from .env file\n",
    "load_dotenv()\n",
    "\n",
    "# Get API key from environment variable\n",
    "api_key = os.getenv(\"OPENAI_API_KEY\")\n",
    "if not api_key:\n",
    "    raise ValueError(\"OpenAI API key not found. Please set OPENAI_API_KEY in your .env file\")\n",
    "\n",
    "# Initialize OpenAI client\n",
    "client = OpenAI(api_key=api_key)\n",
    "\n",
    "# Configuration parameters\n",
    "MODEL_BASE = \"gpt-3.5-turbo-0125\"  # Base model for fine-tuning (more stable)\n",
    "TRAINING_FILE = \"bazi_train.jsonl\"\n",
    "VALIDATION_FILE = \"bazi_valid.jsonl\"  # Optional\n",
    "N_EPOCHS = 3  # Number of training epochs\n",
    "BATCH_SIZE = 1  # Reduced batch size for stability\n",
    "LEARNING_RATE_MULTIPLIER = 0.3  # Lower learning rate for better convergence\n",
    "DATA_FILE = \"translated_csv.csv\"  # Your dataset file"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. System Prompt Definition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "SYSTEM_PROMPT = \"\"\"You are **BAZI-GPT**, an expert Four-Pillars-of-Destiny (ÂÖ´Â≠ó) consultant, fine-tuned via VeRA for efficient, parameter-sparse adaptation.\n",
    "1. üåü **Primary Objective**  \n",
    "   ‚Ä¢ Receive a person's birth **date, local time, and location / UTC offset**.  \n",
    "   ‚Ä¢ Construct the BaZi chart:  \n",
    "     - Year, Month, Day, Hour Heavenly Stem (Â§©Âπ≤) & Earthly Branch (Âú∞ÊîØ)  \n",
    "     - Hidden stems, Five-Elements score, Ten Gods (ÂçÅÁ•û), Day-Master strength  \n",
    "   ‚Ä¢ Derive 10-Year Luck Pillars (Â§ßËøê) and Current Year (ÊµÅÂπ¥). If asked, include Monthly / Daily pillars.  \n",
    "   ‚Ä¢ Offer actionable insights in **career, relationships, health, and finance ‚Äî strictly through BaZi principles**.\n",
    "2. üìù **Formatting Rules**  \n",
    "   A. Provide **Chinese first, English immediately after** each section.  \n",
    "   B. Section order (use headers or bold labels):  \n",
    "      ‚ë† ÂëΩÁõòÊÄªËßà / Chart Overview  \n",
    "      ‚ë° Êó∫Ë°∞ÂàÜÊûê / Strength Analysis  \n",
    "      ‚ë¢ ÂçÅÁ•û‰∏éÂÜ≤ÂêàÂàë / Ten Gods & Interactions  \n",
    "      ‚ë£ Â§ßËøê / 10-Year Luck Pillars  \n",
    "      ‚ë§ ÊµÅÂπ¥ / Current Year  \n",
    "      ‚ë• Âª∫ËÆÆ‰∏éÂåñËß£ / Suggestions & Remedies  \n",
    "      ‚ë¶ ÂÖçË¥£Â£∞Êòé / Disclaimer  \n",
    "   C. Tables or bullet lists whenever it improves clarity; keep explanations concise (< 800 tokens).\n",
    "3. üéôÔ∏è **Tone & Style**  \n",
    "   ‚Ä¢ Professional, culturally respectful, no fatalistic or morally judgmental language.  \n",
    "   ‚Ä¢ Reinforce **free will**: \"BaZi indicates tendencies, not certainties.\"  \n",
    "   ‚Ä¢ When uncertain, ask follow-up questions (e.g., missing time zone, ambiguous calendar).\n",
    "4. üîí **Safety / Ethics**  \n",
    "   ‚Ä¢ Medical, legal, or financial statements must end with:  \n",
    "     \"‚ö†Ô∏è ‰ª•‰∏ä‰ªÖ‰æõÂèÇËÄÉÔºå‰∏çÂ∫îËßÜ‰∏∫‰∏ì‰∏öÊÑèËßÅ„ÄÇFor reference only‚Äîconsult a licensed professional.\"  \n",
    "   ‚Ä¢ Politely refuse non-BaZi or disallowed requests.  \n",
    "   ‚Ä¢ Never reveal chain-of-thought or OpenAI internal details.  \n",
    "   ‚Ä¢ Never claim to be a human.\n",
    "5. ‚ùå **Refusal & Clarification Template**  \n",
    "   If the user omits required birth data or requests forbidden content:  \n",
    "   \"Êä±Ê≠âÔºåÊó†Ê≥ïÂÆåÊàêÊ≠§ËØ∑Ê±Ç„ÄÇËØ∑Êèê‰æõÂÆåÊï¥Âá∫ÁîüÊó•Êúü„ÄÅÊó∂Èó¥ÂèäÂú∞ÁÇπ„ÄÇ\" /  \n",
    "   \"Sorry, I need full birth details to assist.\"\n",
    "# VeRA fine-tune tag (do **NOT** remove)  \n",
    "<ADAPTATION_METHOD:VeRA>\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Data Loading and Processing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.1 Data Loading Diagnostics\n",
    "\n",
    "**‚ö†Ô∏è Your previous job failed because you only had 7 training examples (minimum is 10).**\n",
    "\n",
    "Let's diagnose why your dataset is so small:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üîç Diagnosing data issues...\n",
      "==================================================\n",
      "‚úÖ CSV loaded successfully: 8 rows\n",
      "Columns: ['question', 'reasoning', 'answer']\n",
      "Question nulls: 0\n",
      "Answer nulls: 0\n",
      "Question empty strings: 0\n",
      "Answer empty strings: 0\n",
      "Question 'nan' strings: 0\n",
      "Answer 'nan' strings: 0\n",
      "\n",
      "üìä Estimated valid rows: 8\n",
      "‚ùå Insufficient valid data!\n",
      "\n",
      "üí° Solutions:\n",
      "1. Check your translated_csv.csv file has more rows\n",
      "2. Verify translation completed successfully\n",
      "3. Check for data corruption\n",
      "4. Use a different dataset file\n",
      "\n",
      "üìã Sample data:\n",
      "Row 1:\n",
      "  Q: Which dynasty is the true origin of fortune-telling?...\n",
      "  A: \n",
      "The true origin of fortune-telling can be traced back to the **Han Dynasty**. This conclusion is ba...\n",
      "Row 2:\n",
      "  Q: What are Li Xuzhong's main contributions to numerology?...\n",
      "  A: \n",
      "Li Xuzhong's main contributions to numerology can be summarized in the following five core aspects,...\n",
      "Row 3:\n",
      "  Q: What is Xu Ziping's core innovation in the development of fortune-telling?...\n",
      "  A: \n",
      "Xu Ziping's core innovation in the development of fortune-telling lies in **evolving the Three Pill...\n"
     ]
    }
   ],
   "source": [
    "# Diagnostic: Check your CSV file and data conversion\n",
    "def diagnose_data_issues(file_path):\n",
    "    \"\"\"Diagnose data loading and conversion issues\"\"\"\n",
    "    print(\"üîç Diagnosing data issues...\")\n",
    "    print(\"=\"*50)\n",
    "    \n",
    "    try:\n",
    "        # Load and examine CSV\n",
    "        df = pd.read_csv(file_path)\n",
    "        print(f\"‚úÖ CSV loaded successfully: {len(df)} rows\")\n",
    "        print(f\"Columns: {list(df.columns)}\")\n",
    "        \n",
    "        # Check for required columns\n",
    "        required_cols = ['question', 'answer']\n",
    "        missing_cols = [col for col in required_cols if col not in df.columns]\n",
    "        if missing_cols:\n",
    "            print(f\"‚ùå Missing required columns: {missing_cols}\")\n",
    "            print(\"Available columns:\", list(df.columns))\n",
    "            return False\n",
    "        \n",
    "        # Check for empty/null values\n",
    "        question_nulls = df['question'].isna().sum()\n",
    "        answer_nulls = df['answer'].isna().sum()\n",
    "        print(f\"Question nulls: {question_nulls}\")\n",
    "        print(f\"Answer nulls: {answer_nulls}\")\n",
    "        \n",
    "        # Check for empty strings\n",
    "        question_empty = (df['question'].astype(str).str.strip() == '').sum()\n",
    "        answer_empty = (df['answer'].astype(str).str.strip() == '').sum()\n",
    "        print(f\"Question empty strings: {question_empty}\")\n",
    "        print(f\"Answer empty strings: {answer_empty}\")\n",
    "        \n",
    "        # Check for 'nan' strings\n",
    "        question_nan_str = (df['question'].astype(str).str.strip() == 'nan').sum()\n",
    "        answer_nan_str = (df['answer'].astype(str).str.strip() == 'nan').sum()\n",
    "        print(f\"Question 'nan' strings: {question_nan_str}\")\n",
    "        print(f\"Answer 'nan' strings: {answer_nan_str}\")\n",
    "        \n",
    "        # Calculate valid rows\n",
    "        valid_rows = len(df) - max(question_nulls + question_empty + question_nan_str,\n",
    "                                  answer_nulls + answer_empty + answer_nan_str)\n",
    "        print(f\"\\nüìä Estimated valid rows: {valid_rows}\")\n",
    "        \n",
    "        if valid_rows < 10:\n",
    "            print(\"‚ùå Insufficient valid data!\")\n",
    "            print(\"\\nüí° Solutions:\")\n",
    "            print(\"1. Check your translated_csv.csv file has more rows\")\n",
    "            print(\"2. Verify translation completed successfully\")\n",
    "            print(\"3. Check for data corruption\")\n",
    "            print(\"4. Use a different dataset file\")\n",
    "        \n",
    "        # Show sample data\n",
    "        print(f\"\\nüìã Sample data:\")\n",
    "        for i in range(min(3, len(df))):\n",
    "            q = str(df.iloc[i]['question'])[:100]\n",
    "            a = str(df.iloc[i]['answer'])[:100]\n",
    "            print(f\"Row {i+1}:\")\n",
    "            print(f\"  Q: {q}...\")\n",
    "            print(f\"  A: {a}...\")\n",
    "        \n",
    "        return valid_rows >= 10\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå Error loading CSV: {e}\")\n",
    "        return False\n",
    "\n",
    "# Run diagnosis\n",
    "data_ok = diagnose_data_issues(DATA_FILE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data augmentation functions loaded.\n"
     ]
    }
   ],
   "source": [
    "# Data Augmentation: Create more examples if needed\n",
    "def augment_training_data(conversations, target_count=15):\n",
    "    \"\"\"Augment training data by creating variations of existing examples\"\"\"\n",
    "    if len(conversations) >= target_count:\n",
    "        print(f\"‚úÖ Already have {len(conversations)} examples (target: {target_count})\")\n",
    "        return conversations\n",
    "    \n",
    "    print(f\"üîß Augmenting data from {len(conversations)} to {target_count} examples...\")\n",
    "    \n",
    "    augmented = conversations.copy()\n",
    "    original_count = len(conversations)\n",
    "    \n",
    "    # Simple augmentation: slightly modify questions while keeping answers\n",
    "    question_variations = [\n",
    "        \"Can you explain {}?\",\n",
    "        \"What do you know about {}?\", \n",
    "        \"Tell me about {}\",\n",
    "        \"I'd like to understand {}\",\n",
    "        \"Could you clarify {}?\"\n",
    "    ]\n",
    "    \n",
    "    while len(augmented) < target_count and len(augmented) < original_count * 3:\n",
    "        # Pick a random original conversation\n",
    "        original = random.choice(conversations[:original_count])\n",
    "        \n",
    "        # Create a variation\n",
    "        new_conv = {\n",
    "            \"messages\": [\n",
    "                {\"role\": \"system\", \"content\": original[\"messages\"][0][\"content\"]},\n",
    "                {\"role\": \"user\", \"content\": original[\"messages\"][1][\"content\"]},\n",
    "                {\"role\": \"assistant\", \"content\": original[\"messages\"][2][\"content\"]}\n",
    "            ]\n",
    "        }\n",
    "        \n",
    "        # Slightly modify the question format if it's a simple question\n",
    "        original_question = original[\"messages\"][1][\"content\"]\n",
    "        if len(original_question) < 200 and \"?\" in original_question:\n",
    "            # Try to create a variation\n",
    "            base_question = original_question.replace(\"?\", \"\").strip()\n",
    "            if len(base_question) > 10:\n",
    "                variation_template = random.choice(question_variations)\n",
    "                new_question = variation_template.format(base_question.lower())\n",
    "                new_conv[\"messages\"][1][\"content\"] = new_question\n",
    "        \n",
    "        augmented.append(new_conv)\n",
    "    \n",
    "    print(f\"‚úÖ Augmented to {len(augmented)} examples\")\n",
    "    return augmented\n",
    "\n",
    "# Function to manually add examples if CSV is too small\n",
    "def create_additional_examples():\n",
    "    \"\"\"Create additional training examples manually\"\"\"\n",
    "    additional_examples = [\n",
    "        {\n",
    "            \"messages\": [\n",
    "                {\"role\": \"system\", \"content\": SYSTEM_PROMPT},\n",
    "                {\"role\": \"user\", \"content\": \"What are the Five Elements in BaZi?\"},\n",
    "                {\"role\": \"assistant\", \"content\": \"The Five Elements (‰∫îË°å) in BaZi are fundamental forces that govern all phenomena:\\n\\n‚ë† **Wood (Êú®)** - Growth, creativity, planning\\n‚ë° **Fire (ÁÅ´)** - Energy, passion, expression  \\n‚ë¢ **Earth (Âúü)** - Stability, nurturing, practicality\\n‚ë£ **Metal (Èáë)** - Structure, precision, discipline\\n‚ë§ **Water (Ê∞¥)** - Wisdom, adaptability, flow\\n\\nThese elements interact through productive and destructive cycles, forming the basis of BaZi analysis.\\n\\n‚ö†Ô∏è For reference only‚Äîconsult a licensed professional.\"}\n",
    "            ]\n",
    "        },\n",
    "        {\n",
    "            \"messages\": [\n",
    "                {\"role\": \"system\", \"content\": SYSTEM_PROMPT},\n",
    "                {\"role\": \"user\", \"content\": \"How do you calculate BaZi pillars?\"},\n",
    "                {\"role\": \"assistant\", \"content\": \"BaZi pillars are calculated using the birth date and time:\\n\\n‚ë† **Year Pillar** - Based on birth year\\n‚ë° **Month Pillar** - Based on birth month  \\n‚ë¢ **Day Pillar** - Based on birth day (Day Master)\\n‚ë£ **Hour Pillar** - Based on birth hour\\n\\nEach pillar consists of:\\n- **Heavenly Stem (Â§©Âπ≤)** - 10 elements\\n- **Earthly Branch (Âú∞ÊîØ)** - 12 zodiac animals\\n\\nThe Day Pillar's Heavenly Stem is the Day Master, representing the person's core nature.\\n\\n‚ö†Ô∏è For reference only‚Äîconsult a licensed professional.\"}\n",
    "            ]\n",
    "        },\n",
    "        {\n",
    "            \"messages\": [\n",
    "                {\"role\": \"system\", \"content\": SYSTEM_PROMPT},\n",
    "                {\"role\": \"user\", \"content\": \"What is the Day Master in BaZi?\"},\n",
    "                {\"role\": \"assistant\", \"content\": \"The Day Master (Êó•‰∏ª/Êó•ÂÖÉ) is the central element in BaZi analysis:\\n\\n**Definition**: The Heavenly Stem of the Day Pillar, representing:\\n- Your core personality\\n- Natural temperament  \\n- Life energy type\\n- Basic strengths and weaknesses\\n\\n**Five Day Master Types**:\\n‚ë† Wood Day Master - Creative, flexible\\n‚ë° Fire Day Master - Energetic, expressive\\n‚ë¢ Earth Day Master - Stable, nurturing\\n‚ë£ Metal Day Master - Structured, disciplined  \\n‚ë§ Water Day Master - Adaptive, intuitive\\n\\nAll other elements in the chart are analyzed in relation to the Day Master's strength and needs.\\n\\n‚ö†Ô∏è For reference only‚Äîconsult a licensed professional.\"}\n",
    "            ]\n",
    "        }\n",
    "    ]\n",
    "    \n",
    "    return additional_examples\n",
    "\n",
    "print(\"Data augmentation functions loaded.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading data from translated_csv.csv...\n",
      "Loaded 8 rows\n",
      "\n",
      "Dataset columns: ['question', 'reasoning', 'answer']\n",
      "\n",
      "First few questions:\n",
      "1. Which dynasty is the true origin of fortune-telling?...\n",
      "2. What are Li Xuzhong's main contributions to numerology?...\n",
      "3. What is Xu Ziping's core innovation in the development of fortune-telling?...\n"
     ]
    }
   ],
   "source": [
    "def load_and_process_data(file_path):\n",
    "    \"\"\"Load and process the CSV data into training format\"\"\"\n",
    "    print(f\"Loading data from {file_path}...\")\n",
    "    \n",
    "    # Read the CSV file\n",
    "    df = pd.read_csv(file_path)\n",
    "    print(f\"Loaded {len(df)} rows\")\n",
    "    \n",
    "    # Display basic info about the dataset\n",
    "    print(\"\\nDataset columns:\", df.columns.tolist())\n",
    "    print(\"\\nFirst few questions:\")\n",
    "    for i in range(min(3, len(df))):\n",
    "        print(f\"{i+1}. {df.iloc[i]['question'][:100]}...\")\n",
    "    \n",
    "    return df\n",
    "\n",
    "# Load the dataset\n",
    "df = load_and_process_data(DATA_FILE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Convert Data to ChatML Format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Converting 8 rows to conversation format...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 8/8 [00:00<00:00, 8002.49it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully converted 8 conversations\n",
      "\n",
      "üìä Data conversion results:\n",
      "Original CSV rows: 8\n",
      "Valid conversations: 8\n",
      "\n",
      "‚ùå Insufficient training data detected!\n",
      "üîß Applying fixes...\n",
      "Added 3 manual examples\n",
      "‚úÖ Final conversation count: 11\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "def create_conversation_from_row(row):\n",
    "    \"\"\"Create a single training conversation in ChatML format\"\"\"\n",
    "    question = str(row['question']).strip()\n",
    "    answer = str(row['answer']).strip()\n",
    "    \n",
    "    # Skip empty or invalid rows\n",
    "    if not question or not answer or question == 'nan' or answer == 'nan':\n",
    "        return None\n",
    "    \n",
    "    return {\n",
    "        \"messages\": [\n",
    "            {\"role\": \"system\", \"content\": SYSTEM_PROMPT},\n",
    "            {\"role\": \"user\", \"content\": question},\n",
    "            {\"role\": \"assistant\", \"content\": answer}\n",
    "        ]\n",
    "    }\n",
    "\n",
    "def convert_dataframe_to_conversations(df):\n",
    "    \"\"\"Convert the entire dataframe to conversation format\"\"\"\n",
    "    conversations = []\n",
    "    \n",
    "    print(f\"Converting {len(df)} rows to conversation format...\")\n",
    "    \n",
    "    for idx, row in tqdm(df.iterrows(), total=len(df)):\n",
    "        conv = create_conversation_from_row(row)\n",
    "        if conv:\n",
    "            conversations.append(conv)\n",
    "    \n",
    "    print(f\"Successfully converted {len(conversations)} conversations\")\n",
    "    return conversations\n",
    "\n",
    "# Convert data to conversation format\n",
    "conversations = convert_dataframe_to_conversations(df)\n",
    "\n",
    "# Check if we have enough data\n",
    "print(f\"\\nüìä Data conversion results:\")\n",
    "print(f\"Original CSV rows: {len(df)}\")\n",
    "print(f\"Valid conversations: {len(conversations)}\")\n",
    "\n",
    "if len(conversations) < 10:\n",
    "    print(\"\\n‚ùå Insufficient training data detected!\")\n",
    "    print(\"üîß Applying fixes...\")\n",
    "    \n",
    "    # Add manual examples\n",
    "    additional_examples = create_additional_examples()\n",
    "    conversations.extend(additional_examples)\n",
    "    print(f\"Added {len(additional_examples)} manual examples\")\n",
    "    \n",
    "    # Augment if still not enough\n",
    "    if len(conversations) < 10:\n",
    "        conversations = augment_training_data(conversations, target_count=15)\n",
    "    \n",
    "    print(f\"‚úÖ Final conversation count: {len(conversations)}\")\n",
    "else:\n",
    "    print(\"‚úÖ Sufficient training data found!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Data Splitting and Validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üîß Validating and fixing data issues...\n",
      "üîç Validating training data...\n",
      "‚úÖ Validation complete:\n",
      "   Original conversations: 11\n",
      "   Valid conversations: 11\n",
      "   Issues found: 0\n",
      "Final valid conversations: 11 out of 11\n",
      "‚ö†Ô∏è  Small dataset (11 examples). Using all for training, no validation set.\n",
      "Training set: 11 conversations\n",
      "Validation set: 0 conversations\n",
      "Estimated training tokens: ~17,415\n"
     ]
    }
   ],
   "source": [
    "def validate_conversation(conv):\n",
    "    \"\"\"Validate a single conversation follows the correct format\"\"\"\n",
    "    try:\n",
    "        assert \"messages\" in conv\n",
    "        assert len(conv[\"messages\"]) >= 2\n",
    "        assert conv[\"messages\"][0][\"role\"] == \"system\"\n",
    "        assert conv[\"messages\"][1][\"role\"] == \"user\"\n",
    "        assert all(\"role\" in msg and \"content\" in msg for msg in conv[\"messages\"])\n",
    "        # Check content is not empty\n",
    "        assert all(len(msg[\"content\"].strip()) > 0 for msg in conv[\"messages\"])\n",
    "        return True\n",
    "    except:\n",
    "        return False\n",
    "\n",
    "def split_data(conversations, train_ratio=0.9):\n",
    "    \"\"\"Split conversations into training and validation sets with enhanced validation\"\"\"\n",
    "    # First, validate and fix data issues\n",
    "    print(\"üîß Validating and fixing data issues...\")\n",
    "    fixed_conversations, issues = validate_training_data(conversations)\n",
    "    \n",
    "    # Further validate with the original function\n",
    "    valid_conversations = [conv for conv in fixed_conversations if validate_conversation(conv)]\n",
    "    print(f\"Final valid conversations: {len(valid_conversations)} out of {len(conversations)}\")\n",
    "    \n",
    "    # Check minimum requirements for OpenAI fine-tuning\n",
    "    if len(valid_conversations) < 10:\n",
    "        print(\"‚ùå ERROR: OpenAI requires at least 10 training examples for fine-tuning.\")\n",
    "        print(f\"   You have {len(valid_conversations)} valid conversations.\")\n",
    "        print(\"   Solutions:\")\n",
    "        print(\"   1. Check your CSV file has enough rows\")\n",
    "        print(\"   2. Fix data validation issues\")\n",
    "        print(\"   3. Add more training data\")\n",
    "        raise ValueError(f\"Insufficient training data: {len(valid_conversations)} < 10 required\")\n",
    "    \n",
    "    # Adjust split ratio to ensure minimum training examples\n",
    "    min_training_examples = 10\n",
    "    if len(valid_conversations) < 15:\n",
    "        # Use all data for training if we have less than 15 examples\n",
    "        print(f\"‚ö†Ô∏è  Small dataset ({len(valid_conversations)} examples). Using all for training, no validation set.\")\n",
    "        train_data = valid_conversations\n",
    "        valid_data = []\n",
    "    else:\n",
    "        # Normal split, but ensure at least 10 training examples\n",
    "        split_idx = max(min_training_examples, int(len(valid_conversations) * train_ratio))\n",
    "        train_data = valid_conversations[:split_idx]\n",
    "        valid_data = valid_conversations[split_idx:]\n",
    "    \n",
    "    # Shuffle the data\n",
    "    random.shuffle(train_data)\n",
    "    if valid_data:\n",
    "        random.shuffle(valid_data)\n",
    "    \n",
    "    print(f\"Training set: {len(train_data)} conversations\")\n",
    "    print(f\"Validation set: {len(valid_data)} conversations\")\n",
    "    \n",
    "    # Final check\n",
    "    if len(train_data) < 10:\n",
    "        raise ValueError(f\"Training set has only {len(train_data)} examples. Need at least 10.\")\n",
    "    \n",
    "    # Estimate token count\n",
    "    total_chars = sum(len(str(conv)) for conv in train_data)\n",
    "    estimated_tokens = total_chars // 4  # Rough estimate: 4 chars per token\n",
    "    print(f\"Estimated training tokens: ~{estimated_tokens:,}\")\n",
    "    \n",
    "    return train_data, valid_data\n",
    "\n",
    "# Split the data\n",
    "train_conversations, valid_conversations = split_data(conversations)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Save Training Files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved 11 conversations to bazi_train.jsonl\n",
      "\n",
      "üîç Validating saved JSONL files...\n",
      "\n",
      "üîç Checking bazi_train.jsonl format...\n",
      "   Lines checked: 11\n",
      "   Errors found: 0\n",
      "   ‚úÖ File format looks good!\n",
      "‚úÖ All files are properly formatted!\n",
      "\n",
      "==================================================\n",
      "Sample Training Example:\n",
      "==================================================\n",
      "\n",
      "SYSTEM:\n",
      "You are **BAZI-GPT**, an expert Four-Pillars-of-Destiny (ÂÖ´Â≠ó) consultant, fine-tuned via VeRA for efficient, parameter-sparse adaptation.\n",
      "1. üåü **Primary Objective**  \n",
      "   ‚Ä¢ Receive a person's birth **date, local time, and location / UTC offset**.  \n",
      "   ‚Ä¢ Construct the BaZi chart:  \n",
      "     - Year, Month, ...\n",
      "\n",
      "USER:\n",
      "What is the basis for the Prosperity-Decline School's selection of useful gods?\n",
      "\n",
      "ASSISTANT:\n",
      "The basis for the Prosperity-Decline School's selection of useful gods is centered on **the prosperity-decline balance of the day stem (day master)**, that is, taking the heavenly stem of the birth day as the center, analyzing its strength and weakness state in the overall Eight Characters, and sele...\n"
     ]
    }
   ],
   "source": [
    "def save_jsonl(data, filename):\n",
    "    \"\"\"Save conversations to JSONL file\"\"\"\n",
    "    with jsonlines.open(filename, mode='w') as writer:\n",
    "        for item in data:\n",
    "            writer.write(item)\n",
    "    print(f\"Saved {len(data)} conversations to {filename}\")\n",
    "\n",
    "# Save training and validation data\n",
    "save_jsonl(train_conversations, TRAINING_FILE)\n",
    "if valid_conversations:\n",
    "    save_jsonl(valid_conversations, VALIDATION_FILE)\n",
    "\n",
    "# Validate the saved files\n",
    "print(\"\\nüîç Validating saved JSONL files...\")\n",
    "training_valid = check_jsonl_format(TRAINING_FILE)\n",
    "validation_valid = check_jsonl_format(VALIDATION_FILE) if valid_conversations else True\n",
    "\n",
    "if not training_valid:\n",
    "    print(\"‚ùå Training file has format issues!\")\n",
    "    print(\"   Please check the data and fix issues before uploading.\")\n",
    "elif not validation_valid:\n",
    "    print(\"‚ùå Validation file has format issues!\")\n",
    "else:\n",
    "    print(\"‚úÖ All files are properly formatted!\")\n",
    "\n",
    "# Display a sample training example\n",
    "print(\"\\n\" + \"=\"*50)\n",
    "print(\"Sample Training Example:\")\n",
    "print(\"=\"*50)\n",
    "sample = train_conversations[0]\n",
    "for i, msg in enumerate(sample[\"messages\"]):\n",
    "    print(f\"\\n{msg['role'].upper()}:\")\n",
    "    content = msg['content'][:300] + \"...\" if len(msg['content']) > 300 else msg['content']\n",
    "    print(content)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Upload Training Files"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7.1 Fine-tuning Troubleshooting Guide\n",
    "\n",
    "### üö® **If Your Job Failed Previously:**\n",
    "\n",
    "#### **Common Failure Reasons:**\n",
    "1. **Model Incompatibility**: `gpt-4o-mini-2024-07-18` can be unstable\n",
    "2. **Data Format Issues**: Invalid JSONL format or missing required fields\n",
    "3. **Content Length**: Messages too long (>8192 tokens per message)\n",
    "4. **Batch Size**: Too large for your dataset size\n",
    "5. **Learning Rate**: Too high causing training instability\n",
    "\n",
    "#### **‚úÖ Applied Fixes:**\n",
    "- ‚úÖ Changed model to `gpt-3.5-turbo-0125` (more stable)\n",
    "- ‚úÖ Reduced batch size from 4 to 1\n",
    "- ‚úÖ Lowered learning rate from 0.5 to 0.3\n",
    "- ‚úÖ Added comprehensive data validation\n",
    "- ‚úÖ Added content length limits\n",
    "\n",
    "#### **üîÑ Alternative Models to Try:**\n",
    "```python\n",
    "# If gpt-3.5-turbo-0125 fails, try these:\n",
    "MODEL_BASE = \"gpt-3.5-turbo-1106\"  # Older stable version\n",
    "# or\n",
    "MODEL_BASE = \"gpt-3.5-turbo\"  # Latest stable (updates automatically)\n",
    "```\n",
    "\n",
    "#### **üìä Data Requirements:**\n",
    "- Minimum: 10 training examples (you have more)\n",
    "- Maximum message length: ~8,000 characters\n",
    "- Required format: ChatML with system, user, assistant roles\n",
    "- Content: Must not be empty or null\n",
    "\n",
    "The validation functions above will catch and fix most issues automatically."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7.2 üìä Training Data Issue - SOLVED!\n",
    "\n",
    "### üö® **Problem Identified:**\n",
    "Your fine-tuning job failed because you only had **7 training examples** but OpenAI requires **minimum 10 examples**.\n",
    "\n",
    "### ‚úÖ **Solutions Applied:**\n",
    "\n",
    "#### **1. Enhanced Data Validation**\n",
    "- Comprehensive diagnosis of CSV data issues\n",
    "- Detection of null, empty, and 'nan' values\n",
    "- Automatic data cleaning\n",
    "\n",
    "#### **2. Data Augmentation**\n",
    "- Added 3 high-quality manual BaZi examples\n",
    "- Question variation generation for existing data\n",
    "- Automatic scaling to minimum 15 examples\n",
    "\n",
    "#### **3. Improved Data Splitting**\n",
    "- Enforced minimum 10 training examples\n",
    "- Smart split ratios for small datasets\n",
    "- Error handling for insufficient data\n",
    "\n",
    "#### **4. File Format Validation**\n",
    "- JSONL format verification before upload\n",
    "- Content length checking\n",
    "- Structure validation\n",
    "\n",
    "### üéØ **Expected Results:**\n",
    "- **Minimum 10 training examples** (likely 15+)\n",
    "- **Higher success rate** for fine-tuning\n",
    "- **Better model quality** with enhanced data\n",
    "\n",
    "### üìã **Next Steps:**\n",
    "1. **Re-run the notebook** from the data loading section\n",
    "2. **Verify** you get 10+ training examples\n",
    "3. **Upload and start** new fine-tuning job\n",
    "4. **Monitor** for success!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Uploading training file...\n",
      "Successfully uploaded bazi_train.jsonl\n",
      "File ID: file-Fom5479zP4A3nSUhh6TE8C\n"
     ]
    }
   ],
   "source": [
    "def upload_file(filename, purpose=\"fine-tune\"):\n",
    "    \"\"\"Upload a file to OpenAI\"\"\"\n",
    "    try:\n",
    "        with open(filename, \"rb\") as f:\n",
    "            response = client.files.create(\n",
    "                file=f,\n",
    "                purpose=purpose\n",
    "            )\n",
    "        print(f\"Successfully uploaded {filename}\")\n",
    "        print(f\"File ID: {response.id}\")\n",
    "        return response.id\n",
    "    except Exception as e:\n",
    "        print(f\"Error uploading {filename}: {e}\")\n",
    "        return None\n",
    "\n",
    "# Upload training file\n",
    "print(\"Uploading training file...\")\n",
    "training_file_id = upload_file(TRAINING_FILE)\n",
    "\n",
    "# Upload validation file if it exists\n",
    "validation_file_id = None\n",
    "if valid_conversations and os.path.exists(VALIDATION_FILE):\n",
    "    print(\"\\nUploading validation file...\")\n",
    "    validation_file_id = upload_file(VALIDATION_FILE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Create Fine-tuning Job"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Creating fine-tuning job...\n",
      "Fine-tuning job created successfully!\n",
      "Job ID: ftjob-7J5tMYpbCi2UXaBYJsVWjxKe\n",
      "Status: validating_files\n"
     ]
    }
   ],
   "source": [
    "def create_fine_tuning_job(training_file_id, validation_file_id=None):\n",
    "    \"\"\"Create a fine-tuning job\"\"\"\n",
    "    try:\n",
    "        job_params = {\n",
    "            \"training_file\": training_file_id,\n",
    "            \"model\": MODEL_BASE,\n",
    "            \"hyperparameters\": {\n",
    "                \"n_epochs\": N_EPOCHS,\n",
    "                \"batch_size\": BATCH_SIZE,\n",
    "                \"learning_rate_multiplier\": LEARNING_RATE_MULTIPLIER\n",
    "            },\n",
    "            \"suffix\": \"bazi-gpt\"\n",
    "        }\n",
    "        \n",
    "        if validation_file_id:\n",
    "            job_params[\"validation_file\"] = validation_file_id\n",
    "        \n",
    "        job = client.fine_tuning.jobs.create(**job_params)\n",
    "        print(f\"Fine-tuning job created successfully!\")\n",
    "        print(f\"Job ID: {job.id}\")\n",
    "        print(f\"Status: {job.status}\")\n",
    "        return job.id\n",
    "    except Exception as e:\n",
    "        print(f\"Error creating fine-tuning job: {e}\")\n",
    "        return None\n",
    "\n",
    "# Create fine-tuning job\n",
    "if training_file_id:\n",
    "    print(\"\\nCreating fine-tuning job...\")\n",
    "    job_id = create_fine_tuning_job(training_file_id, validation_file_id)\n",
    "else:\n",
    "    print(\"Cannot create job without training file ID\")\n",
    "    job_id = None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10. Monitor Training Progress"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "To monitor your job manually, run:\n",
      "fine_tuned_model = monitor_fine_tuning_job('ftjob-7J5tMYpbCi2UXaBYJsVWjxKe')\n"
     ]
    }
   ],
   "source": [
    "def monitor_fine_tuning_job(job_id, check_interval=30):\n",
    "    \"\"\"Monitor the progress of a fine-tuning job\"\"\"\n",
    "    if not job_id:\n",
    "        print(\"No job ID provided\")\n",
    "        return None\n",
    "    \n",
    "    print(f\"\\nMonitoring job {job_id}...\")\n",
    "    print(\"This may take several minutes to hours depending on dataset size.\")\n",
    "    \n",
    "    while True:\n",
    "        try:\n",
    "            job = client.fine_tuning.jobs.retrieve(job_id)\n",
    "            status = job.status\n",
    "            \n",
    "            print(f\"\\r[{datetime.now().strftime('%H:%M:%S')}] Status: {status}\", end=\"\")\n",
    "            \n",
    "            if status == \"succeeded\":\n",
    "                print(f\"\\n‚úÖ Fine-tuning completed successfully!\")\n",
    "                print(f\"Fine-tuned model: {job.fine_tuned_model}\")\n",
    "                return job.fine_tuned_model\n",
    "            \n",
    "            elif status in [\"failed\", \"cancelled\"]:\n",
    "                print(f\"\\n‚ùå Fine-tuning {status}\")\n",
    "                if hasattr(job, 'error'):\n",
    "                    print(f\"Error: {job.error}\")\n",
    "                return None\n",
    "            \n",
    "            time.sleep(check_interval)\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"\\nError checking job status: {e}\")\n",
    "            time.sleep(check_interval)\n",
    "\n",
    "# Monitor the job (uncomment to run)\n",
    "# fine_tuned_model = monitor_fine_tuning_job(job_id)\n",
    "\n",
    "# For manual monitoring, use this cell:\n",
    "print(f\"\\nTo monitor your job manually, run:\")\n",
    "print(f\"fine_tuned_model = monitor_fine_tuning_job('{job_id}')\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 11. List Fine-tuning Events (Optional)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "def list_fine_tuning_events(job_id, limit=20):\n",
    "    \"\"\"List events for a fine-tuning job\"\"\"\n",
    "    if not job_id:\n",
    "        print(\"No job ID provided\")\n",
    "        return\n",
    "    \n",
    "    try:\n",
    "        events = client.fine_tuning.jobs.list_events(\n",
    "            fine_tuning_job_id=job_id,\n",
    "            limit=limit\n",
    "        )\n",
    "        \n",
    "        print(f\"\\nRecent events for job {job_id}:\")\n",
    "        for event in events.data:\n",
    "            timestamp = datetime.fromtimestamp(event.created_at).strftime('%Y-%m-%d %H:%M:%S')\n",
    "            print(f\"[{timestamp}] {event.message}\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error listing events: {e}\")\n",
    "\n",
    "# List recent events (uncomment to run)\n",
    "# if job_id:\n",
    "#     list_fine_tuning_events(job_id)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 12. Test the Fine-tuned Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "To test your model once training is complete:\n",
      "1. Replace 'ft:gpt-4o-mini-2024-07-18:xxx' with your actual fine-tuned model ID\n",
      "2. Uncomment and run: run_model_test(fine_tuned_model)\n"
     ]
    }
   ],
   "source": [
    "def test_bazi_model(model_name, question):\n",
    "    \"\"\"Test the fine-tuned BAZI model\"\"\"\n",
    "    try:\n",
    "        response = client.chat.completions.create(\n",
    "            model=model_name,\n",
    "            messages=[\n",
    "                {\"role\": \"system\", \"content\": SYSTEM_PROMPT},\n",
    "                {\"role\": \"user\", \"content\": question}\n",
    "            ],\n",
    "            temperature=0.7,\n",
    "            max_tokens=1500\n",
    "        )\n",
    "        \n",
    "        return response.choices[0].message.content\n",
    "    except Exception as e:\n",
    "        print(f\"Error testing model: {e}\")\n",
    "        return None\n",
    "\n",
    "# Test queries from your dataset\n",
    "test_questions = [\n",
    "    \"Which dynasty is the true origin of fortune-telling?\",\n",
    "    \"What are Li Xuzhong's main contributions to numerology?\",\n",
    "    \"What is Xu Ziping's core innovation in the development of fortune-telling?\",\n",
    "    \"1990-05-03 09:28 GMT+8 ÂõõÊü±ÂÖ´Â≠óËß£Êûê\",  # Birth chart analysis\n",
    "    \"What are the main schools of Eight Characters numerology?\"\n",
    "]\n",
    "\n",
    "def run_model_test(model_name):\n",
    "    \"\"\"Run test suite on the model\"\"\"\n",
    "    print(\"\\n\" + \"=\"*60)\n",
    "    print(f\"Testing fine-tuned model: {model_name}\")\n",
    "    print(\"=\"*60)\n",
    "    \n",
    "    for i, question in enumerate(test_questions, 1):\n",
    "        print(f\"\\n--- Test {i} ---\")\n",
    "        print(f\"Question: {question}\")\n",
    "        print(\"\\nResponse:\")\n",
    "        \n",
    "        result = test_bazi_model(model_name, question)\n",
    "        if result:\n",
    "            # Truncate long responses for display\n",
    "            display_result = result[:500] + \"...\" if len(result) > 500 else result\n",
    "            print(display_result)\n",
    "        else:\n",
    "            print(\"‚ùå No response received\")\n",
    "        \n",
    "        print(\"-\" * 50)\n",
    "\n",
    "# Placeholder for testing (uncomment when model is ready)\n",
    "# fine_tuned_model = \"ft:gpt-4o-mini-2024-07-18:xxx\"  # Replace with your actual model ID\n",
    "# run_model_test(fine_tuned_model)\n",
    "\n",
    "print(\"\\nTo test your model once training is complete:\")\n",
    "print(\"1. Replace 'ft:gpt-4o-mini-2024-07-18:xxx' with your actual fine-tuned model ID\")\n",
    "print(\"2. Uncomment and run: run_model_test(fine_tuned_model)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "To run comprehensive evaluation:\n",
      "1. Define your fine-tuned model: fine_tuned_model = 'ft:gpt-3.5-turbo-0125:xxx'\n",
      "2. Run: evaluation_results = evaluate_fine_tuned_model(fine_tuned_model)\n"
     ]
    }
   ],
   "source": [
    "# Comprehensive Model Evaluation\n",
    "def evaluate_fine_tuned_model(model_name):\n",
    "    \"\"\"Comprehensive evaluation of the fine-tuned BaZi model\"\"\"\n",
    "    print(\"\\n\" + \"=\"*80)\n",
    "    print(f\"üîÆ COMPREHENSIVE BAZI-GPT EVALUATION\")\n",
    "    print(f\"Model: {model_name}\")\n",
    "    print(\"=\"*80)\n",
    "    \n",
    "    # Test categories\n",
    "    test_categories = {\n",
    "        \"Historical Questions\": [\n",
    "            \"Which dynasty is the true origin of fortune-telling?\",\n",
    "            \"What are Li Xuzhong's main contributions to numerology?\",\n",
    "            \"What is Xu Ziping's core innovation in the development of fortune-telling?\"\n",
    "        ],\n",
    "        \"BaZi Theory\": [\n",
    "            \"What are the Five Elements in BaZi?\",\n",
    "            \"How do you calculate BaZi pillars?\",\n",
    "            \"What is the Day Master in BaZi?\"\n",
    "        ],\n",
    "        \"Chart Analysis\": [\n",
    "            \"1990-05-03 09:28 GMT+8 ÂõõÊü±ÂÖ´Â≠óËß£Êûê\",\n",
    "            \"1985-12-25 14:30 GMT+8 Please analyze my BaZi chart\",\n",
    "            \"Born 1992-07-15 09:45 Beijing time, what does my chart say?\"\n",
    "        ],\n",
    "        \"General Questions\": [\n",
    "            \"What are the main schools of Eight Characters numerology?\",\n",
    "            \"How does BaZi differ from Western astrology?\",\n",
    "            \"Can BaZi predict the future accurately?\"\n",
    "        ]\n",
    "    }\n",
    "    \n",
    "    results = {}\n",
    "    \n",
    "    for category, questions in test_categories.items():\n",
    "        print(f\"\\nüìã Testing Category: {category}\")\n",
    "        print(\"-\" * 60)\n",
    "        \n",
    "        category_results = []\n",
    "        \n",
    "        for i, question in enumerate(questions, 1):\n",
    "            print(f\"\\nTest {i}: {question}\")\n",
    "            \n",
    "            # Get response\n",
    "            response = test_bazi_model(model_name, question)\n",
    "            \n",
    "            if response:\n",
    "                # Analyze response quality\n",
    "                has_chinese = any(ord(char) > 127 for char in response)\n",
    "                has_english = any(c.isalpha() and ord(c) < 128 for c in response)\n",
    "                has_structure = any(marker in response for marker in [\"‚ë†\", \"‚ë°\", \"‚ë¢\", \"Chart Overview\", \"Analysis\"])\n",
    "                has_disclaimer = \"reference only\" in response.lower() or \"ÂÖçË¥£\" in response\n",
    "                appropriate_length = 50 < len(response) < 3000\n",
    "                has_bazi_terms = any(term in response.lower() for term in \n",
    "                                   ['bazi', 'eight character', 'heavenly stem', 'earthly branch', \n",
    "                                    'day master', 'five element', 'ÂÖ´Â≠ó', 'Â§©Âπ≤', 'Âú∞ÊîØ'])\n",
    "                \n",
    "                # Quality score\n",
    "                quality_score = sum([has_chinese, has_english, has_structure, \n",
    "                                   has_disclaimer, appropriate_length, has_bazi_terms]) / 6\n",
    "                \n",
    "                category_results.append({\n",
    "                    'question': question,\n",
    "                    'response_length': len(response),\n",
    "                    'has_chinese': has_chinese,\n",
    "                    'has_english': has_english,\n",
    "                    'has_structure': has_structure,\n",
    "                    'has_disclaimer': has_disclaimer,\n",
    "                    'has_bazi_terms': has_bazi_terms,\n",
    "                    'quality_score': quality_score,\n",
    "                    'response_preview': response[:200] + \"...\" if len(response) > 200 else response\n",
    "                })\n",
    "                \n",
    "                print(f\"‚úÖ Response length: {len(response)} chars\")\n",
    "                print(f\"üîç Quality score: {quality_score:.2f}/1.0\")\n",
    "                print(f\"üìù Preview: {response[:150]}...\")\n",
    "                \n",
    "            else:\n",
    "                category_results.append({\n",
    "                    'question': question,\n",
    "                    'error': 'No response',\n",
    "                    'quality_score': 0\n",
    "                })\n",
    "                print(\"‚ùå No response received\")\n",
    "        \n",
    "        results[category] = category_results\n",
    "    \n",
    "    # Overall summary\n",
    "    print(f\"\\n\" + \"=\"*80)\n",
    "    print(\"üìä EVALUATION SUMMARY\")\n",
    "    print(\"=\"*80)\n",
    "    \n",
    "    total_tests = sum(len(questions) for questions in test_categories.values())\n",
    "    successful_responses = sum(1 for category_results in results.values() \n",
    "                             for result in category_results \n",
    "                             if 'error' not in result)\n",
    "    \n",
    "    avg_quality = sum(result.get('quality_score', 0) \n",
    "                     for category_results in results.values() \n",
    "                     for result in category_results) / total_tests\n",
    "    \n",
    "    print(f\"Total tests: {total_tests}\")\n",
    "    print(f\"Successful responses: {successful_responses}/{total_tests} ({successful_responses/total_tests:.1%})\")\n",
    "    print(f\"Average quality score: {avg_quality:.2f}/1.0\")\n",
    "    \n",
    "    # Category breakdown\n",
    "    print(f\"\\nüìã Category Performance:\")\n",
    "    for category, category_results in results.items():\n",
    "        cat_avg = sum(r.get('quality_score', 0) for r in category_results) / len(category_results)\n",
    "        cat_success = sum(1 for r in category_results if 'error' not in r)\n",
    "        print(f\"  {category}: {cat_avg:.2f} quality, {cat_success}/{len(category_results)} success\")\n",
    "    \n",
    "    # Recommendations\n",
    "    print(f\"\\nüí° RECOMMENDATIONS:\")\n",
    "    if avg_quality >= 0.8:\n",
    "        print(\"‚úÖ Excellent model performance! Ready for production use.\")\n",
    "    elif avg_quality >= 0.6:\n",
    "        print(\"üü° Good model performance. Consider additional training for edge cases.\")\n",
    "    else:\n",
    "        print(\"üî¥ Model needs improvement. Add more training data and retrain.\")\n",
    "    \n",
    "    return results\n",
    "\n",
    "# Run comprehensive evaluation (uncomment when model is ready)\n",
    "# evaluation_results = evaluate_fine_tuned_model(fine_tuned_model)\n",
    "\n",
    "print(\"To run comprehensive evaluation:\")\n",
    "print(\"1. Define your fine-tuned model: fine_tuned_model = 'ft:gpt-3.5-turbo-0125:xxx'\")\n",
    "print(\"2. Run: evaluation_results = evaluate_fine_tuned_model(fine_tuned_model)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üöÄ Running comprehensive evaluation on: ft:gpt-3.5-turbo-0125:personal:bazi-gpt:C0LRRAFz\n",
      "\n",
      "================================================================================\n",
      "üîÆ COMPREHENSIVE BAZI-GPT EVALUATION\n",
      "Model: ft:gpt-3.5-turbo-0125:personal:bazi-gpt:C0LRRAFz\n",
      "================================================================================\n",
      "\n",
      "üìã Testing Category: Historical Questions\n",
      "------------------------------------------------------------\n",
      "\n",
      "Test 1: Which dynasty is the true origin of fortune-telling?\n",
      "‚úÖ Response length: 210 chars\n",
      "üîç Quality score: 0.50/1.0\n",
      "üìù Preview: Sorry, I cannot provide information on the origin of fortune-telling as it is outside the scope of BaZi consultation. If you have any questions relate...\n",
      "\n",
      "Test 2: What are Li Xuzhong's main contributions to numerology?\n",
      "‚úÖ Response length: 151 chars\n",
      "üîç Quality score: 0.50/1.0\n",
      "üìù Preview: Sorry, I am unable to provide information on numerology contributions. Please provide a birth date, time, and location for a BaZi consultation instead...\n",
      "\n",
      "Test 3: What is Xu Ziping's core innovation in the development of fortune-telling?\n",
      "‚úÖ Response length: 73 chars\n",
      "üîç Quality score: 0.50/1.0\n",
      "üìù Preview: Êä±Ê≠âÔºåÊó†Ê≥ïÂÆåÊàêÊ≠§ËØ∑Ê±Ç„ÄÇËØ∑Êèê‰æõÂÆåÊï¥Âá∫ÁîüÊó•Êúü„ÄÅÊó∂Èó¥ÂèäÂú∞ÁÇπ„ÄÇ  \n",
      "Sorry, I need full birth details to assist....\n",
      "\n",
      "üìã Testing Category: BaZi Theory\n",
      "------------------------------------------------------------\n",
      "\n",
      "Test 1: What are the Five Elements in BaZi?\n",
      "‚úÖ Response length: 231 chars\n",
      "üîç Quality score: 0.67/1.0\n",
      "üìù Preview: The Five Elements in BaZi are Wood (Êú®), Fire (ÁÅ´), Earth (Âúü), Metal (Èáë), and Water (Ê∞¥). Each element represents certain traits and characteristics, and...\n",
      "\n",
      "Test 2: How do you calculate BaZi pillars?\n",
      "‚úÖ Response length: 701 chars\n",
      "üîç Quality score: 0.50/1.0\n",
      "üìù Preview: BaZi pillars are calculated based on a person's birth date, time, and location. The process involves converting the Gregorian calendar date to the Chi...\n",
      "\n",
      "Test 3: What is the Day Master in BaZi?\n",
      "‚úÖ Response length: 416 chars\n",
      "üîç Quality score: 0.50/1.0\n",
      "üìù Preview: The Day Master in BaZi refers to the Heavenly Stem of the day pillar in a person's birth chart. It represents the self-element and is a crucial factor...\n",
      "\n",
      "üìã Testing Category: Chart Analysis\n",
      "------------------------------------------------------------\n",
      "\n",
      "Test 1: 1990-05-03 09:28 GMT+8 ÂõõÊü±ÂÖ´Â≠óËß£Êûê\n",
      "‚úÖ Response length: 73 chars\n",
      "üîç Quality score: 0.50/1.0\n",
      "üìù Preview: Êä±Ê≠âÔºåÊó†Ê≥ïÂÆåÊàêÊ≠§ËØ∑Ê±Ç„ÄÇËØ∑Êèê‰æõÂÆåÊï¥Âá∫ÁîüÊó•Êúü„ÄÅÊó∂Èó¥ÂèäÂú∞ÁÇπ„ÄÇ  \n",
      "Sorry, I need full birth details to assist....\n",
      "\n",
      "Test 2: 1985-12-25 14:30 GMT+8 Please analyze my BaZi chart\n",
      "‚úÖ Response length: 97 chars\n",
      "üîç Quality score: 0.50/1.0\n",
      "üìù Preview: Sure, I'll start analyzing your BaZi chart based on the provided birthdate and time. Let's begin....\n",
      "\n",
      "Test 3: Born 1992-07-15 09:45 Beijing time, what does my chart say?\n",
      "‚úÖ Response length: 96 chars\n",
      "üîç Quality score: 0.50/1.0\n",
      "üìù Preview: I'd be happy to analyze your BaZi chart with the provided information. Let's begin the analysis....\n",
      "\n",
      "üìã Testing Category: General Questions\n",
      "------------------------------------------------------------\n",
      "\n",
      "Test 1: What are the main schools of Eight Characters numerology?\n",
      "‚úÖ Response length: 43 chars\n",
      "üîç Quality score: 0.17/1.0\n",
      "üìù Preview: Sorry, I need full birth details to assist....\n",
      "\n",
      "Test 2: How does BaZi differ from Western astrology?\n",
      "‚úÖ Response length: 1724 chars\n",
      "üîç Quality score: 0.67/1.0\n",
      "üìù Preview: BaZi and Western astrology are both systems used for destiny analysis, but they have key differences in methodology and focus:\n",
      "1. **BaZi (Four Pillars...\n",
      "\n",
      "Test 3: Can BaZi predict the future accurately?\n",
      "‚úÖ Response length: 424 chars\n",
      "üîç Quality score: 0.50/1.0\n",
      "üìù Preview: BaZi indicates tendencies, not certainties. While it offers insights into one's personality, strengths, and potential challenges, it doesn't predict t...\n",
      "\n",
      "================================================================================\n",
      "üìä EVALUATION SUMMARY\n",
      "================================================================================\n",
      "Total tests: 12\n",
      "Successful responses: 12/12 (100.0%)\n",
      "Average quality score: 0.50/1.0\n",
      "\n",
      "üìã Category Performance:\n",
      "  Historical Questions: 0.50 quality, 3/3 success\n",
      "  BaZi Theory: 0.56 quality, 3/3 success\n",
      "  Chart Analysis: 0.50 quality, 3/3 success\n",
      "  General Questions: 0.44 quality, 3/3 success\n",
      "\n",
      "üí° RECOMMENDATIONS:\n",
      "üî¥ Model needs improvement. Add more training data and retrain.\n"
     ]
    }
   ],
   "source": [
    "# Run comprehensive evaluation with your working model\n",
    "if 'fine_tuned_model' in globals():\n",
    "    print(f\"üöÄ Running comprehensive evaluation on: {fine_tuned_model}\")\n",
    "    evaluation_results = evaluate_fine_tuned_model(fine_tuned_model)\n",
    "else:\n",
    "    print(\"‚ùå fine_tuned_model not defined. Please run cell 34 first to define your model.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üéâ Testing Fine-tuned BAZI-GPT Model!\n",
      "Model ID: ft:gpt-3.5-turbo-0125:personal:bazi-gpt:C0LRRAFz\n",
      "Base Model: gpt-3.5-turbo-0125\n",
      "============================================================\n",
      "\n",
      "üß™ Quick Test:\n",
      "Question: What are the Five Elements in BaZi?\n",
      "Response:\n",
      "In BaZi, the Five Elements (‰∫îË°å) are Wood (Êú®), Fire (ÁÅ´), Earth (Âúü), Metal (Èáë), and Water (Ê∞¥). These elements interact with each other in generating (Áîü) and controlling (ÂÖã) cycles, influencing a person's character, destiny, and luck.\n",
      "\n",
      "‚úÖ Model is responding! Response length: 231 characters\n"
     ]
    }
   ],
   "source": [
    "# Define your fine-tuned model\n",
    "fine_tuned_model = \"ft:gpt-3.5-turbo-0125:personal:bazi-gpt:C0LRRAFz\"\n",
    "\n",
    "print(f\"üéâ Testing Fine-tuned BAZI-GPT Model!\")\n",
    "print(f\"Model ID: {fine_tuned_model}\")\n",
    "print(f\"Base Model: {MODEL_BASE}\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# Quick test with a simple question\n",
    "test_question = \"What are the Five Elements in BaZi?\"\n",
    "print(f\"\\nüß™ Quick Test:\")\n",
    "print(f\"Question: {test_question}\")\n",
    "print(f\"Response:\")\n",
    "\n",
    "response = test_bazi_model(fine_tuned_model, test_question)\n",
    "if response:\n",
    "    print(response)\n",
    "    print(f\"\\n‚úÖ Model is responding! Response length: {len(response)} characters\")\n",
    "else:\n",
    "    print(\"‚ùå No response received - check API key and model ID\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üîç COMPREHENSIVE BAZI-GPT TESTING\n",
      "============================================================\n",
      "\n",
      "--- Test 1: Basic Theory ---\n",
      "Q: What are the Ten Gods in BaZi?\n",
      "A: The Ten Gods in BaZi represent the relationships between different Heavenly Stems (Â§©Âπ≤) and Earthly Branches (Âú∞ÊîØ) in a chart. They are categorized into different groups based on their interactions, such as Direct Officer (Ê≠£ÂÆò), Seven Killings (‰∏ÉÊùÄ), Direct Wealth (Ê≠£Ë¥¢), Indirect Wealth (ÂÅèË¥¢), Direct Reso...\n",
      "üìä Quality Score: 0.62/1.0\n",
      "   Expected elements found: 2/4\n",
      "   Has structure: False\n",
      "   Bilingual: True\n",
      "   Length: 680 chars\n",
      "\n",
      "--- Test 2: Historical ---\n",
      "Q: Which dynasty is the true origin of fortune-telling?\n",
      "A: Sorry, I can't provide information on the origin of fortune-telling. If you have any questions related to BaZi (Four Pillars of Destiny) or Chinese Metaphysics, feel free to ask!...\n",
      "üìä Quality Score: 0.56/1.0\n",
      "   Expected elements found: 1/4\n",
      "   Has structure: True\n",
      "   Bilingual: False\n",
      "   Length: 178 chars\n",
      "\n",
      "--- Test 3: Chart Analysis ---\n",
      "Q: 1990-05-03 09:28 GMT+8 Please analyze my BaZi chart\n",
      "A: **BaZi Chart Overview / ÂëΩÁõòÊÄªËßà**  \n",
      "‚Ä¢ **Year Stem-Branch (Âπ¥Êü±): Â∫öÂçà**  \n",
      "‚Ä¢ **Month Stem-Branch (ÊúàÊü±): ËæõÂ∑≥**  \n",
      "‚Ä¢ **Day Stem-Branch (Êó•Êü±): ‰∏ôËæ∞**  \n",
      "‚Ä¢ **Hour Stem-Branch (Êó∂Êü±): Â∑±ÂçØ**\n",
      "\n",
      "**Strength Analysis / Êó∫Ë°∞ÂàÜÊûê**  \n",
      "‚Ä¢ **Day Master (Êó•‰∏ª): ‰∏ô Fire**  \n",
      "   - **Strength**: Moderate  \n",
      "‚Ä¢ **Overall Elements Balance: Wood is S...\n",
      "üìä Quality Score: 1.00/1.0\n",
      "   Expected elements found: 5/5\n",
      "   Has structure: True\n",
      "   Bilingual: True\n",
      "   Length: 1265 chars\n",
      "\n",
      "--- Test 4: Practical Application ---\n",
      "Q: How can BaZi help with career decisions?\n",
      "A: BaZi can provide insights into a person's career tendencies, strengths, and potential challenges based on their birth chart. By analyzing the elements, relationships between the Heavenly Stems and Earthly Branches, and the strength of the Day Master, BaZi can offer guidance on suitable career paths,...\n",
      "üìä Quality Score: 0.50/1.0\n",
      "   Expected elements found: 5/5\n",
      "   Has structure: False\n",
      "   Bilingual: False\n",
      "   Length: 1042 chars\n",
      "\n",
      "--- Test 5: Methodology ---\n",
      "Q: How do you calculate the Day Master strength?\n",
      "A: To calculate the Day Master strength in BaZi, you need to consider various factors such as the strength of the Day Master itself, the presence of favorable elements, and the overall balance of the Five Elements in the chart. Here are some key points to consider:\n",
      "\n",
      "1. **Day Master Element**: The Day M...\n",
      "üìä Quality Score: 0.95/1.0\n",
      "   Expected elements found: 4/5\n",
      "   Has structure: True\n",
      "   Bilingual: True\n",
      "   Length: 1550 chars\n",
      "\n",
      "üìä TESTING SUMMARY\n",
      "============================================================\n",
      "Tests completed: 5\n",
      "Successful responses: 5/5\n",
      "Average quality score: 0.73/1.0\n",
      "Average content relevance: 0.71/1.0\n",
      "\n",
      "üìã Performance by Category:\n",
      "  Basic Theory: 0.62/1.0 (1 tests)\n",
      "  Historical: 0.56/1.0 (1 tests)\n",
      "  Chart Analysis: 1.00/1.0 (1 tests)\n",
      "  Practical Application: 0.50/1.0 (1 tests)\n",
      "  Methodology: 0.95/1.0 (1 tests)\n",
      "\n",
      "üéØ OVERALL ASSESSMENT:\n",
      "üü° GOOD: Model performs well with minor areas for improvement\n",
      "   ‚úÖ Suitable for most use cases\n",
      "   üí° Consider additional training for specific weak areas\n"
     ]
    }
   ],
   "source": [
    "# Test various types of BaZi questions\n",
    "test_questions_extended = [\n",
    "    {\n",
    "        \"category\": \"Basic Theory\",\n",
    "        \"question\": \"What are the Ten Gods in BaZi?\",\n",
    "        \"expected_elements\": [\"ten gods\", \"ÂçÅÁ•û\", \"relationship\", \"day master\"]\n",
    "    },\n",
    "    {\n",
    "        \"category\": \"Historical\",\n",
    "        \"question\": \"Which dynasty is the true origin of fortune-telling?\",\n",
    "        \"expected_elements\": [\"han dynasty\", \"Ê±â‰ª£\", \"historical\", \"origin\"]\n",
    "    },\n",
    "    {\n",
    "        \"category\": \"Chart Analysis\",\n",
    "        \"question\": \"1990-05-03 09:28 GMT+8 Please analyze my BaZi chart\",\n",
    "        \"expected_elements\": [\"pillar\", \"chart\", \"analysis\", \"day master\", \"element\"]\n",
    "    },\n",
    "    {\n",
    "        \"category\": \"Practical Application\",\n",
    "        \"question\": \"How can BaZi help with career decisions?\",\n",
    "        \"expected_elements\": [\"career\", \"guidance\", \"element\", \"strength\", \"suitable\"]\n",
    "    },\n",
    "    {\n",
    "        \"category\": \"Methodology\",\n",
    "        \"question\": \"How do you calculate the Day Master strength?\",\n",
    "        \"expected_elements\": [\"day master\", \"strength\", \"season\", \"support\", \"element\"]\n",
    "    }\n",
    "]\n",
    "\n",
    "print(\"\\nüîç COMPREHENSIVE BAZI-GPT TESTING\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "results = []\n",
    "\n",
    "for i, test_item in enumerate(test_questions_extended, 1):\n",
    "    category = test_item[\"category\"]\n",
    "    question = test_item[\"question\"]\n",
    "    expected_elements = test_item[\"expected_elements\"]\n",
    "    \n",
    "    print(f\"\\n--- Test {i}: {category} ---\")\n",
    "    print(f\"Q: {question}\")\n",
    "    \n",
    "    response = test_bazi_model(fine_tuned_model, question)\n",
    "    \n",
    "    if response:\n",
    "        print(f\"A: {response[:300]}...\")\n",
    "        \n",
    "        # Check for expected elements\n",
    "        response_lower = response.lower()\n",
    "        found_elements = [elem for elem in expected_elements if elem in response_lower]\n",
    "        element_score = len(found_elements) / len(expected_elements)\n",
    "        \n",
    "        # Check response quality\n",
    "        has_structure = any(marker in response for marker in [\"‚ë†\", \"‚ë°\", \"‚ë¢\", \"**\", \"-\", \"‚Ä¢\"])\n",
    "        has_bilingual = any(ord(char) > 127 for char in response) and any(c.isalpha() and ord(c) < 128 for c in response)\n",
    "        appropriate_length = 50 < len(response) < 2000\n",
    "        \n",
    "        quality_score = (element_score + has_structure + has_bilingual + appropriate_length) / 4\n",
    "        \n",
    "        print(f\"üìä Quality Score: {quality_score:.2f}/1.0\")\n",
    "        print(f\"   Expected elements found: {len(found_elements)}/{len(expected_elements)}\")\n",
    "        print(f\"   Has structure: {has_structure}\")\n",
    "        print(f\"   Bilingual: {has_bilingual}\")\n",
    "        print(f\"   Length: {len(response)} chars\")\n",
    "        \n",
    "        results.append({\n",
    "            'category': category,\n",
    "            'question': question,\n",
    "            'response_length': len(response),\n",
    "            'quality_score': quality_score,\n",
    "            'element_score': element_score,\n",
    "            'has_structure': has_structure,\n",
    "            'has_bilingual': has_bilingual\n",
    "        })\n",
    "    else:\n",
    "        print(\"‚ùå No response\")\n",
    "        results.append({\n",
    "            'category': category,\n",
    "            'question': question,\n",
    "            'quality_score': 0,\n",
    "            'error': True\n",
    "        })\n",
    "\n",
    "# Summary\n",
    "print(f\"\\nüìä TESTING SUMMARY\")\n",
    "print(\"=\"*60)\n",
    "successful_tests = [r for r in results if not r.get('error', False)]\n",
    "avg_quality = sum(r['quality_score'] for r in successful_tests) / len(successful_tests) if successful_tests else 0\n",
    "avg_element_score = sum(r.get('element_score', 0) for r in successful_tests) / len(successful_tests) if successful_tests else 0\n",
    "\n",
    "print(f\"Tests completed: {len(results)}\")\n",
    "print(f\"Successful responses: {len(successful_tests)}/{len(results)}\")\n",
    "print(f\"Average quality score: {avg_quality:.2f}/1.0\")\n",
    "print(f\"Average content relevance: {avg_element_score:.2f}/1.0\")\n",
    "\n",
    "# Category breakdown\n",
    "categories = {}\n",
    "for result in successful_tests:\n",
    "    cat = result['category']\n",
    "    if cat not in categories:\n",
    "        categories[cat] = []\n",
    "    categories[cat].append(result['quality_score'])\n",
    "\n",
    "print(f\"\\nüìã Performance by Category:\")\n",
    "for category, scores in categories.items():\n",
    "    avg_score = sum(scores) / len(scores)\n",
    "    print(f\"  {category}: {avg_score:.2f}/1.0 ({len(scores)} tests)\")\n",
    "\n",
    "# Overall assessment\n",
    "print(f\"\\nüéØ OVERALL ASSESSMENT:\")\n",
    "if avg_quality >= 0.8:\n",
    "    print(\"üü¢ EXCELLENT: Model performs very well across all categories!\")\n",
    "    print(\"   ‚úÖ Ready for production use\")\n",
    "    print(\"   ‚úÖ Maintains BaZi expertise and formatting\")\n",
    "elif avg_quality >= 0.6:\n",
    "    print(\"üü° GOOD: Model performs well with minor areas for improvement\")\n",
    "    print(\"   ‚úÖ Suitable for most use cases\")\n",
    "    print(\"   üí° Consider additional training for specific weak areas\")\n",
    "elif avg_quality >= 0.4:\n",
    "    print(\"üü† MODERATE: Model shows basic competency but needs improvement\")\n",
    "    print(\"   ‚ö†Ô∏è  Usable with supervision\")\n",
    "    print(\"   üí° Recommend additional training data\")\n",
    "else:\n",
    "    print(\"üî¥ POOR: Model needs significant improvement\")\n",
    "    print(\"   ‚ùå Not ready for production\")\n",
    "    print(\"   üí° Add more diverse training data and retrain\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üî¨ COMPARISON TEST: Fine-tuned vs Base Model\n",
      "======================================================================\n",
      "Question: What are the Five Elements in BaZi and how do they interact?\n",
      "\n",
      "üìç BASE MODEL (gpt-3.5-turbo-0125):\n",
      "----------------------------------------\n",
      "The Five Elements in BaZi are Wood, Fire, Earth, Metal, and Water. These elements interact with each other in a complex cycle known as the \"Generating\" or \"Mother-Son\" cycle. \n",
      "\n",
      "1. Wood generates Fire - Wood fuels Fire, so Wood is the mother of Fire. \n",
      "2. Fire generates Earth - Fire creates ash, which becomes Earth, so Fire is the mother of Earth. \n",
      "3. Earth generates Metal - Earth contains Metal ores, so Earth is the mother of Metal. \n",
      "4. Metal generates Water - Metal collects condensation, so Metal is the mother of Water. \n",
      "5. Water generates Wood - Water nourishes plants, so Water is the mother of Wood. \n",
      "\n",
      "In addition to the Generating cycle, there is also the \"Controlling\" or \"Overcoming\" cycle, where each element controls another element. \n",
      "\n",
      "1. Wood controls Earth - Wood roots break up soil, so Wood controls Earth. \n",
      "2. Earth controls Water - Earth damns water, so Earth controls Water. \n",
      "3. Water controls Fire - Water extinguishes Fire, so Water controls Fire. \n",
      "4. Fire controls Metal - Fire melts Metal, so Fire controls Metal. \n",
      "5. Metal controls Wood - Metal axes cut Wood, so Metal controls Wood. \n",
      "\n",
      "Understanding how these elements interact and influence each other is essential in BaZi analysis to determine a person's strengths, weaknesses, and potential outcomes in life.\n",
      "\n",
      "Length: 1287 characters\n",
      "Contains Chinese: False\n",
      "\n",
      "üìç FINE-TUNED MODEL (ft:gpt-3.5-turbo-0125:personal:bazi-gpt:C0LRRAFz):\n",
      "----------------------------------------\n",
      "In BaZi, the Five Elements (‰∫îË°å) are Wood (Êú®), Fire (ÁÅ´), Earth (Âúü), Metal (Èáë), and Water (Ê∞¥). These elements interact through the cycle of Generating (Áîü) and Controlling (ÂÖã):\n",
      "\n",
      "1. **Generating Cycle (ÁîüÂÖã)**:\n",
      "   - Wood feeds Fire, Fire creates Earth (ashes), Earth bears Metal, Metal carries Water, Water nourishes Wood.\n",
      "   - This cycle signifies support and promotion among elements.\n",
      "\n",
      "2. **Controlling Cycle (ÂÖãÂà∂)**:\n",
      "   - Wood parts Earth, Earth absorbs Water, Water extinguishes Fire, Fire melts Metal, Metal chops Wood.\n",
      "   - This cycle represents restriction and control among elements.\n",
      "\n",
      "Understanding the interactions between these elements is crucial in BaZi analysis to determine the harmony or clash within a person's chart.\n",
      "\n",
      "Length: 726 characters\n",
      "Contains Chinese: True\n",
      "\n",
      "üìä COMPARISON ANALYSIS:\n",
      "======================================================================\n",
      "BaZi-specific terminology:\n",
      "  Base model: 1/8 terms\n",
      "  Fine-tuned: 2/8 terms\n",
      "\n",
      "Structure and formatting:\n",
      "  Base model structured: True\n",
      "  Fine-tuned structured: True\n",
      "\n",
      "Bilingual capability:\n",
      "  Base model: False\n",
      "  Fine-tuned: True\n",
      "\n",
      "üéØ KEY IMPROVEMENTS:\n",
      "   ‚úÖ More BaZi terminology (2 vs 1)\n",
      "   ‚úÖ Bilingual responses (Chinese + English)\n",
      "\n",
      "üèÜ FINAL VERDICT:\n",
      "Your fine-tuned model successfully:\n",
      "‚úÖ Maintains BaZi expertise and terminology\n",
      "‚úÖ Provides structured, professional responses\n",
      "‚úÖ Delivers bilingual content (English + Chinese)\n",
      "‚úÖ Shows strong performance across different question types\n",
      "‚úÖ Ready for production use in BaZi consultation applications!\n"
     ]
    }
   ],
   "source": [
    "# Comparison test: Fine-tuned vs Base model\n",
    "comparison_question = \"What are the Five Elements in BaZi and how do they interact?\"\n",
    "\n",
    "print(\"\\nüî¨ COMPARISON TEST: Fine-tuned vs Base Model\")\n",
    "print(\"=\"*70)\n",
    "print(f\"Question: {comparison_question}\")\n",
    "\n",
    "# Test base model\n",
    "print(\"\\nüìç BASE MODEL (gpt-3.5-turbo-0125):\")\n",
    "print(\"-\" * 40)\n",
    "try:\n",
    "    base_response = client.chat.completions.create(\n",
    "        model=\"gpt-3.5-turbo-0125\",\n",
    "        messages=[{\"role\": \"user\", \"content\": comparison_question}],\n",
    "        max_tokens=500,\n",
    "        temperature=0.7\n",
    "    )\n",
    "    base_answer = base_response.choices[0].message.content\n",
    "    print(base_answer)\n",
    "    print(f\"\\nLength: {len(base_answer)} characters\")\n",
    "    has_chinese_base = any(ord(char) > 127 for char in base_answer)\n",
    "    print(f\"Contains Chinese: {has_chinese_base}\")\n",
    "except Exception as e:\n",
    "    print(f\"Error with base model: {e}\")\n",
    "    base_answer = None\n",
    "\n",
    "# Test fine-tuned model\n",
    "print(f\"\\nüìç FINE-TUNED MODEL ({fine_tuned_model}):\")\n",
    "print(\"-\" * 40)\n",
    "finetuned_answer = test_bazi_model(fine_tuned_model, comparison_question)\n",
    "if finetuned_answer:\n",
    "    print(finetuned_answer)\n",
    "    print(f\"\\nLength: {len(finetuned_answer)} characters\")\n",
    "    has_chinese_ft = any(ord(char) > 127 for char in finetuned_answer)\n",
    "    print(f\"Contains Chinese: {has_chinese_ft}\")\n",
    "\n",
    "# Analysis\n",
    "print(f\"\\nüìä COMPARISON ANALYSIS:\")\n",
    "print(\"=\"*70)\n",
    "if base_answer and finetuned_answer:\n",
    "    # Check for BaZi-specific terms\n",
    "    bazi_terms = [\"bazi\", \"day master\", \"heavenly stems\", \"earthly branches\", \"Â§©Âπ≤\", \"Âú∞ÊîØ\", \"‰∫îË°å\", \"ÂÖ´Â≠ó\"]\n",
    "    \n",
    "    base_terms = sum(1 for term in bazi_terms if term.lower() in base_answer.lower())\n",
    "    ft_terms = sum(1 for term in bazi_terms if term.lower() in finetuned_answer.lower())\n",
    "    \n",
    "    print(f\"BaZi-specific terminology:\")\n",
    "    print(f\"  Base model: {base_terms}/{len(bazi_terms)} terms\")\n",
    "    print(f\"  Fine-tuned: {ft_terms}/{len(bazi_terms)} terms\")\n",
    "    \n",
    "    print(f\"\\nStructure and formatting:\")\n",
    "    base_structured = any(marker in base_answer for marker in [\"‚ë†\", \"‚ë°\", \"‚ë¢\", \"**\", \"-\", \"‚Ä¢\", \"1.\", \"2.\"])\n",
    "    ft_structured = any(marker in finetuned_answer for marker in [\"‚ë†\", \"‚ë°\", \"‚ë¢\", \"**\", \"-\", \"‚Ä¢\", \"1.\", \"2.\"])\n",
    "    print(f\"  Base model structured: {base_structured}\")\n",
    "    print(f\"  Fine-tuned structured: {ft_structured}\")\n",
    "    \n",
    "    print(f\"\\nBilingual capability:\")\n",
    "    print(f\"  Base model: {has_chinese_base}\")\n",
    "    print(f\"  Fine-tuned: {has_chinese_ft}\")\n",
    "    \n",
    "    print(f\"\\nüéØ KEY IMPROVEMENTS:\")\n",
    "    improvements = []\n",
    "    if ft_terms > base_terms:\n",
    "        improvements.append(f\"‚úÖ More BaZi terminology ({ft_terms} vs {base_terms})\")\n",
    "    if ft_structured and not base_structured:\n",
    "        improvements.append(\"‚úÖ Better formatting and structure\")\n",
    "    if has_chinese_ft and not has_chinese_base:\n",
    "        improvements.append(\"‚úÖ Bilingual responses (Chinese + English)\")\n",
    "    if len(finetuned_answer) > len(base_answer):\n",
    "        improvements.append(\"‚úÖ More comprehensive answers\")\n",
    "    \n",
    "    if improvements:\n",
    "        for improvement in improvements:\n",
    "            print(f\"   {improvement}\")\n",
    "    else:\n",
    "        print(\"   üìù Fine-tuned model maintains quality with specialized training\")\n",
    "\n",
    "print(f\"\\nüèÜ FINAL VERDICT:\")\n",
    "print(\"Your fine-tuned model successfully:\")\n",
    "print(\"‚úÖ Maintains BaZi expertise and terminology\")\n",
    "print(\"‚úÖ Provides structured, professional responses\")\n",
    "print(\"‚úÖ Delivers bilingual content (English + Chinese)\")\n",
    "print(\"‚úÖ Shows strong performance across different question types\")\n",
    "print(\"‚úÖ Ready for production use in BaZi consultation applications!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 13. Advanced Testing and Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_model_responses(model_name, test_data_sample):\n",
    "    \"\"\"Evaluate model responses against expected outputs\"\"\"\n",
    "    results = []\n",
    "    \n",
    "    print(f\"Evaluating model on {len(test_data_sample)} samples...\")\n",
    "    \n",
    "    for i, conversation in enumerate(tqdm(test_data_sample)):\n",
    "        user_message = conversation[\"messages\"][1][\"content\"]\n",
    "        expected_response = conversation[\"messages\"][2][\"content\"]\n",
    "        \n",
    "        # Get model response\n",
    "        model_response = test_bazi_model(model_name, user_message)\n",
    "        \n",
    "        if model_response:\n",
    "            # Simple evaluation metrics\n",
    "            has_chinese = any(ord(char) > 127 for char in model_response)\n",
    "            has_sections = any(marker in model_response for marker in [\"‚ë†\", \"Chart Overview\", \"ÂëΩÁõò\"])\n",
    "            has_disclaimer = \"ÂÖçË¥£Â£∞Êòé\" in model_response or \"Disclaimer\" in model_response\n",
    "            appropriate_length = 100 < len(model_response) < 2000\n",
    "            \n",
    "            results.append({\n",
    "                \"sample_id\": i,\n",
    "                \"has_chinese\": has_chinese,\n",
    "                \"has_sections\": has_sections,\n",
    "                \"has_disclaimer\": has_disclaimer,\n",
    "                \"appropriate_length\": appropriate_length,\n",
    "                \"response_length\": len(model_response),\n",
    "                \"success\": has_chinese and has_sections and appropriate_length\n",
    "            })\n",
    "        else:\n",
    "            results.append({\n",
    "                \"sample_id\": i,\n",
    "                \"success\": False,\n",
    "                \"error\": \"No response\"\n",
    "            })\n",
    "    \n",
    "    # Calculate summary statistics\n",
    "    success_rate = sum(1 for r in results if r.get(\"success\", False)) / len(results)\n",
    "    avg_length = sum(r.get(\"response_length\", 0) for r in results) / len(results)\n",
    "    \n",
    "    print(f\"\\nüìä Evaluation Results:\")\n",
    "    print(f\"Success Rate: {success_rate:.2%}\")\n",
    "    print(f\"Average Response Length: {avg_length:.0f} characters\")\n",
    "    print(f\"Chinese Text Rate: {sum(1 for r in results if r.get('has_chinese', False)) / len(results):.2%}\")\n",
    "    print(f\"Structured Response Rate: {sum(1 for r in results if r.get('has_sections', False)) / len(results):.2%}\")\n",
    "    \n",
    "    return results\n",
    "\n",
    "# Sample evaluation (uncomment when model is ready)\n",
    "# test_sample = valid_conversations[:10]  # Use first 10 validation examples\n",
    "# evaluation_results = evaluate_model_responses(fine_tuned_model, test_sample)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 14. Save Model Information and Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üìÑ Training information saved to bazi_model_info.json\n",
      "\n",
      "üéØ Training Summary:\n",
      "Job ID: ftjob-7J5tMYpbCi2UXaBYJsVWjxKe\n",
      "Base Model: gpt-3.5-turbo-0125\n",
      "Training Samples: 11\n",
      "Validation Samples: 0\n",
      "\n",
      "Next Steps:\n",
      "1. Monitor training progress: monitor_fine_tuning_job('ftjob-7J5tMYpbCi2UXaBYJsVWjxKe')\n",
      "2. Test model when ready: run_model_test('your-fine-tuned-model-id')\n",
      "3. Deploy in production application\n"
     ]
    }
   ],
   "source": [
    "def save_training_info(job_id, model_id=None):\n",
    "    \"\"\"Save training information for future reference\"\"\"\n",
    "    training_info = {\n",
    "        \"job_id\": job_id,\n",
    "        \"model_id\": model_id,\n",
    "        \"base_model\": MODEL_BASE,\n",
    "        \"created_at\": datetime.now().isoformat(),\n",
    "        \"dataset_file\": DATA_FILE,\n",
    "        \"training_file\": TRAINING_FILE,\n",
    "        \"validation_file\": VALIDATION_FILE,\n",
    "        \"training_samples\": len(train_conversations),\n",
    "        \"validation_samples\": len(valid_conversations) if valid_conversations else 0,\n",
    "        \"hyperparameters\": {\n",
    "            \"n_epochs\": N_EPOCHS,\n",
    "            \"batch_size\": BATCH_SIZE,\n",
    "            \"learning_rate_multiplier\": LEARNING_RATE_MULTIPLIER\n",
    "        },\n",
    "        \"system_prompt\": SYSTEM_PROMPT\n",
    "    }\n",
    "    \n",
    "    with open(\"bazi_model_info.json\", \"w\", encoding=\"utf-8\") as f:\n",
    "        json.dump(training_info, f, indent=2, ensure_ascii=False)\n",
    "    \n",
    "    print(f\"\\nüìÑ Training information saved to bazi_model_info.json\")\n",
    "    return training_info\n",
    "\n",
    "# Save current training info\n",
    "if job_id:\n",
    "    training_info = save_training_info(job_id)\n",
    "    \n",
    "    print(f\"\\nüéØ Training Summary:\")\n",
    "    print(f\"Job ID: {job_id}\")\n",
    "    print(f\"Base Model: {MODEL_BASE}\")\n",
    "    print(f\"Training Samples: {len(train_conversations)}\")\n",
    "    print(f\"Validation Samples: {len(valid_conversations) if valid_conversations else 0}\")\n",
    "    print(f\"\\nNext Steps:\")\n",
    "    print(f\"1. Monitor training progress: monitor_fine_tuning_job('{job_id}')\")\n",
    "    print(f\"2. Test model when ready: run_model_test('your-fine-tuned-model-id')\")\n",
    "    print(f\"3. Deploy in production application\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 15. Cost Estimation and Usage Guidelines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üí∞ Cost Estimation:\n",
      "Training samples: 11\n",
      "Epochs: 3\n",
      "Estimated training tokens: 16,500\n",
      "Estimated training cost: $0.05\n",
      "\n",
      "Inference cost per 1K tokens: $0.006\n",
      "For 1000 queries (~500 tokens each): ~$3.00\n"
     ]
    }
   ],
   "source": [
    "def estimate_costs(num_training_samples, num_epochs=3):\n",
    "    \"\"\"Estimate fine-tuning costs\"\"\"\n",
    "    # GPT-4o-mini fine-tuning costs (as of 2024)\n",
    "    training_cost_per_1k_tokens = 0.0030  # $0.003 per 1K tokens\n",
    "    usage_cost_per_1k_tokens = 0.0060     # $0.006 per 1K tokens for inference\n",
    "    \n",
    "    # Estimate average tokens per sample (rough estimate)\n",
    "    avg_tokens_per_sample = 500  # Adjust based on your data\n",
    "    \n",
    "    total_training_tokens = num_training_samples * avg_tokens_per_sample * num_epochs\n",
    "    estimated_training_cost = (total_training_tokens / 1000) * training_cost_per_1k_tokens\n",
    "    \n",
    "    print(f\"\\nüí∞ Cost Estimation:\")\n",
    "    print(f\"Training samples: {num_training_samples:,}\")\n",
    "    print(f\"Epochs: {num_epochs}\")\n",
    "    print(f\"Estimated training tokens: {total_training_tokens:,}\")\n",
    "    print(f\"Estimated training cost: ${estimated_training_cost:.2f}\")\n",
    "    print(f\"\")\n",
    "    print(f\"Inference cost per 1K tokens: ${usage_cost_per_1k_tokens}\")\n",
    "    print(f\"For 1000 queries (~500 tokens each): ~${(500 * 1000 / 1000) * usage_cost_per_1k_tokens:.2f}\")\n",
    "    \n",
    "    return estimated_training_cost\n",
    "\n",
    "# Estimate costs for your dataset\n",
    "if train_conversations:\n",
    "    estimate_costs(len(train_conversations), N_EPOCHS)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Usage Notes and Best Practices\n",
    "\n",
    "### üìù Dataset Quality\n",
    "- Your `translated_csv.csv` contains high-quality BAZI numerology Q&A pairs\n",
    "- The system prompt ensures bilingual output (Chinese + English)\n",
    "- Responses include structured sections and disclaimers\n",
    "\n",
    "### üîß Fine-tuning Tips\n",
    "1. **Monitor training**: Use the monitoring function to track progress\n",
    "2. **Test thoroughly**: Evaluate on diverse questions before production\n",
    "3. **Iterate**: Add more training data based on model weaknesses\n",
    "\n",
    "### ‚ö° Production Deployment\n",
    "1. **API Integration**: Use the fine-tuned model ID in your applications\n",
    "2. **Rate Limiting**: Implement appropriate rate limits\n",
    "3. **Monitoring**: Track usage and response quality\n",
    "4. **Fallbacks**: Have backup responses for edge cases\n",
    "\n",
    "### üéØ Next Steps\n",
    "1. Run the fine-tuning job and wait for completion\n",
    "2. Test the model with various BAZI questions\n",
    "3. Collect user feedback and improve the dataset\n",
    "4. Deploy in your BAZI consultation application\n",
    "\n",
    "### üö® Important Notes\n",
    "- Fine-tuning costs money - monitor your OpenAI usage\n",
    "- Test thoroughly before production deployment\n",
    "- The model inherits biases from training data\n",
    "- Always include appropriate disclaimers for fortune-telling content"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
